{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4621eb2-5791-4283-99e2-b094337d53c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have our data, let's start cleaning and formatting. This notebook focuses on the Python Pandas library.<br>\n",
    "<br>N.B. When viewing the columns, note the following prefixes:<br>\n",
    "_v :\tCalculated value (max, min or mean)<br>\n",
    "_s :\tStandard deviation of mean<br>\n",
    "_c :\tCount of (number of) values included<br>\n",
    "_d :\tDate range for values<br>\n",
    "_y :\tYears where extreme occurred (limited to first 40) <br><br>\n",
    "(https://www.weatherstats.ca/faq/#download-columns-ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d1401c-d2a5-44e7-aa0c-ce58a5679358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Load the raw data \"\"\"\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dbfs/mnt/weather/raw/weather_data.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a35347-aaa6-4e69-a3d4-6c699177ddf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Overview of the data \"\"\"\n",
    "\n",
    "# view the fields\n",
    "print(df.columns)\n",
    "\n",
    "# view the row count and column count\n",
    "print(df.shape)\n",
    "\n",
    "# # view first 5 rows\n",
    "display(df.head())\n",
    "\n",
    "# # view last 5 rows\n",
    "display(df.tail())\n",
    "\n",
    "# view summary stats\n",
    "# display(df.info())\n",
    "display(df.describe())  # other way to view stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f766d2-d642-4361-b5a5-9e0bd572ea52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# more advanced summary stats, unique to Databricks\n",
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd336c6-8540-45c7-aaa0-d96892a3236f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Select relevant columns and fix the data types \"\"\"\n",
    "\n",
    "# verify data types\n",
    "display(df.dtypes)\n",
    "\n",
    "\n",
    "# select desired columns, rename them, and fix the data types\n",
    "df_clean = df.rename(\n",
    "    columns={\n",
    "        \"max_dew_point_v\": \"max_dew_point\",\n",
    "        \"max_relative_humidity_v\": \"max_relative_humidity\",\n",
    "        \"max_temperature_v\": \"max_temperature\",\n",
    "        \"max_wind_speed_v\": \"max_wind_speed\",\n",
    "        \"min_dew_point_v\": \"min_dew_point\",\n",
    "        \"min_relative_humidity_v\": \"min_relative_humidity\",\n",
    "        \"min_temperature_v\": \"min_temperature\",\n",
    "        \"min_wind_speed_v\": \"min_wind_speed\",\n",
    "        \"precipitation_v\": \"precipitation\",\n",
    "        \"snow_v\": \"snow\",\n",
    "        \"snow_on_ground_v\": \"snow_on_ground\",\n",
    "    }\n",
    ")[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"max_dew_point\",\n",
    "        \"max_relative_humidity\",\n",
    "        \"max_temperature\",\n",
    "        \"max_wind_speed\",\n",
    "        \"min_dew_point\",\n",
    "        \"min_relative_humidity\",\n",
    "        \"min_temperature\",\n",
    "        \"min_wind_speed\",\n",
    "        \"precipitation\",\n",
    "        \"snow\",\n",
    "        \"snow_on_ground\",\n",
    "    ]\n",
    "].astype(\n",
    "    {\n",
    "        \"date\": \"datetime64\",\n",
    "        \"max_dew_point\": \"float\",\n",
    "        \"max_relative_humidity\": \"float\",\n",
    "        \"max_temperature\": \"float\",\n",
    "        \"max_wind_speed\": \"float\",\n",
    "        \"min_dew_point\": \"float\",\n",
    "        \"min_relative_humidity\": \"float\",\n",
    "        \"min_temperature\": \"float\",\n",
    "        \"min_wind_speed\": \"float\",\n",
    "        \"precipitation\": \"float\",\n",
    "        \"snow\": \"float\",\n",
    "        \"snow_on_ground\": \"float\",\n",
    "    }\n",
    ")\n",
    "df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"]).dt.date\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ec8e6b-44c2-4194-bdca-9339817a5b66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Handle missing values \"\"\"\n",
    "\n",
    "# import\n",
    "from datetime import date\n",
    "\n",
    "# find the number of null rows\n",
    "print(df_clean.isna().sum())\n",
    "\n",
    "# exclude the 'date' column and find the number of rows that have null in every column\n",
    "df_subset = df_clean.drop(\"date\", axis=1)\n",
    "print(df_subset.isnull().all(axis=1).sum())\n",
    "\n",
    "# there are 360 rows with nulls in every column, let's now see what they are\n",
    "display(df_clean[df_subset.isnull().all(axis=1)])\n",
    "\n",
    "# we see that the data is null starting from 1872-06-23 and anything earlier.\n",
    "df_clean = df_clean[\n",
    "    df_clean[\"date\"] > date(1872, 6, 23)\n",
    "]  # drop the rows that have a 'date' before 1872-06-23\n",
    "# print(df_clean.isna().sum())\n",
    "\n",
    "# We can now safely replace the remaining nulls with 0.\n",
    "df_clean.fillna(0, inplace=True)\n",
    "\n",
    "# verify that there are no null values left\n",
    "print(df_clean.isna().sum())\n",
    "\n",
    "# view the row count and column count\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a51f115-0232-42f5-926b-c29033df6649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Handle duplicated rows \"\"\"\n",
    "\n",
    "# verify if there are any duplicates in our current dataframe\n",
    "print(\"Duplicated rows of the whole dataframe: \", df_clean.duplicated().sum())\n",
    "\n",
    "# No duplicates, however let's create a subset of the data that excludes the 'date' column\n",
    "df_subset = df_clean.drop(\"date\", axis=1)\n",
    "\n",
    "# within that subset, find the amount of rows that are duplicated\n",
    "print(\"Duplicated rows of the subset: \", df_subset.duplicated().sum())\n",
    "\n",
    "# now we see that there are 32 rows that are duplicates, let's see what they are\n",
    "df3 = df_clean[df_subset.duplicated(keep=False)]\n",
    "# display(df3)\n",
    "\n",
    "# the duplicated rows are not shown together so let's sort the result\n",
    "display(df3.sort_values(by=\"date\"))\n",
    "\n",
    "# we see that the duplicates are mostly focused on very early dates and often are due to a mismatched digit. It is safe to drop them.\n",
    "df_clean = df_clean[~df_subset.duplicated()]\n",
    "\n",
    "\n",
    "# view the row count and column count\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bce3d8-65ba-4b7c-9c85-5c4d8a7bb34e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Export the processed dataframe to storage \"\"\"\n",
    "\n",
    "# write the result file to processed folder, save as parquet file\n",
    "df_clean.to_parquet(\n",
    "    \"/dbfs/mnt/weather/processed/weather_processed.parquet\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Data Processing with Python Pandas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
